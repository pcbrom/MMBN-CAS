)
# === STEP 5: Compute reliability ===
records <- list()
# 5.1 Full scale – all profiles
res <- reliability_for(item_cols, db)
records <- append(records, list(
tibble(cut = "full_scale", profile = "all", dimension = "all",
n_items = 22, alpha = res$alpha, omega = res$omega)
))
# 5.2 Full scale – by profile
for (prof in unique(db$profile)) {
subset_prof <- filter(db, profile == prof)
res <- reliability_for(item_cols, subset_prof)
records <- append(records, list(
tibble(cut = "full_scale", profile = prof, dimension = "all",
n_items = 22, alpha = res$alpha, omega = res$omega)
))
}
# 5.3 Each dimension (overall + by profile)
for (dim in names(dimensions)) {
cols <- dimensions[[dim]]
# overall
res <- reliability_for(cols, db)
records <- append(records, list(
tibble(cut = "dimension", profile = "all", dimension = dim,
n_items = length(cols), alpha = res$alpha, omega = res$omega)
))
# by profile
for (prof in unique(db$profile)) {
subset_prof <- filter(db, profile == prof)
res <- reliability_for(cols, subset_prof)
records <- append(records, list(
tibble(cut = "dimension", profile = prof, dimension = dim,
n_items = length(cols), alpha = res$alpha, omega = res$omega)
))
}
# === STEP 6: Consolidate and display ===
reliability_df <- bind_rows(records) |>
mutate(across(c(alpha, omega), \(x) round(x, digits = 3)))
print(reliability_df)
# === STEP 1: Load libraries ===
library(psych)
library(GPArotation)
library(tidyverse)
# === STEP 2: Definir itens ===
item_cols <- paste0("q", 1:22)
item_mat <- db[, item_cols] |> drop_na()
# === STEP 3: Parallel Analysis using Spearman ===
cor_spear <- cor(item_mat, method = "spearman", use = "pairwise.complete.obs")
# Valores próprios reais
eig_real <- eigen(cor_spear)$values
# Simulação paralela com permutação dos dados
set.seed(123)
n_iter <- 1000
eig_rand <- matrix(NA, nrow = n_iter, ncol = length(item_cols))
for (i in 1:n_iter) {
rand <- sample(unlist(item_mat), size = length(unlist(item_mat)), replace = TRUE)
rand_mat <- matrix(rand, ncol = ncol(item_mat))
colnames(rand_mat) <- colnames(item_mat)
eig_rand[i, ] <- eigen(cor(cor(rand_mat, method = "spearman"))) $values
}
mean_rand <- colMeans(eig_rand)
# Plot
plot(1:22, eig_real, type = "o", pch = 16, col = "blue",
xlab = "Factor", ylab = "Eigenvalue", main = "Parallel Analysis – Spearman")
lines(1:22, mean_rand, type = "o", lty = 2, col = "red")
abline(v = 3, lty = 3)
legend("topright", legend = c("Actual (Spearman)", "Parallel mean"),
col = c("blue", "red"), lty = c(1, 2), pch = 16)
# === STEP 4: EFA with 3 factors (ML, Promax rotation) ===
efa <- fa(item_mat, nfactors = 3, fm = "ml", rotate = "promax")
# === STEP 5: Mostrar apenas cargas salientes (|λ| ≥ 0.30) ===
load_df <- as.data.frame(round(efa$loadings[1:22, ], 3))
salient <- load_df
salient[abs(salient) < 0.30] <- '-'
cat("\nSalient loadings (|λ| ≥ 0.30):\n")
print(salient)
# === STEP 1: Load library ===
library(lavaan)
library(dplyr)
# === STEP 2: Modelo CFA com 3 fatores ===
model_cfa <- "
F1 =~ q1 + q2 + q3 + q4 + q5 + q6
F2 =~ q7 + q8 + q9 + q10 + q11 + q12
F3 =~ q13 + q14 + q15 + q16 + q17 + q18 + q19 + q20 + q21 + q22
"
# === STEP 3: Ajuste com DWLS e dados ordinais ===
fit <- sem(model_cfa, data = db, estimator = "WLSMV", ordered = colnames(db)[grepl("^q\\d+$", colnames(db))])
# === STEP 4: Extração dos índices de modificação ===
mi <- modindices(fit)
# === STEP 5: Filtro para correlações residuais entre itens (q* ~~ q*) ===
mi_errors <- mi %>%
filter(op == "~~", grepl("^q\\d+$", lhs), grepl("^q\\d+$", rhs)) %>%
arrange(desc(mi))
# === STEP 6: Mostrar os 10 maiores ===
print(mi_errors[1:10, c("lhs", "rhs", "mi")])
# === STEP 1: Load lavaan ===
library(lavaan)
# === STEP 2: Definir o modelo bifatorial ===
model_bifactor <- "
G =~ q1 + q2 + q3 + q4 + q5 + q6 + q7 + q8 + q9 + q10 + q11 + q12 + q13 + q14 + q15 + q16 + q17 + q18 + q19 + q20 + q21 + q22
F1 =~ q1 + q2 + q3 + q4 + q5 + q6
F2 =~ q7 + q8 + q9 + q10 + q11 + q12
F3 =~ q13 + q14 + q15 + q16 + q17 + q18 + q19 + q20 + q21 + q22
G ~~ 0*F1 + 0*F2 + 0*F3
F1 ~~ 0*F2 + 0*F3
F2 ~~ 0*F3
"
# === STEP 3: Ajustar modelo com DWLS ===
fit_bifactor <- sem(model_bifactor,
data = db,
estimator = "WLSMV",
ordered = colnames(db)[grepl("^q\\d+$", colnames(db))])
# === STEP 4: Exibir resumo com medidas de ajuste ===
summary(fit_bifactor, fit.measures = TRUE, standardized = TRUE)
# === STEP 1: Carregar pacotes ===
library(mirt)
# === STEP 2: Ajustar modelo GRM unidimensional ===
model_grm <- mirt(db[, paste0("q", 1:22)],
model = 1,
itemtype = "graded",
SE = TRUE)
# === STEP 3: Mostrar parâmetros (discriminações e thresholds) ===
summary(model_grm)
# Diretório de saída (modifique conforme necessário)
output_dir <- "outputs/"
# Cria curva de informação por item
png(filename = file.path(output_dir, "item_information.png"), width = 800, height = 600)
plot(model_grm, type = "info")
dev.off()
# Cria curva de informação total com erro padrão
png(filename = file.path(output_dir, "test_information.png"), width = 800, height = 600)
plot(model_grm, type = "infoSE")
dev.off()
library(gridExtra)
library(grid)
library(ggplotify)
install.packages('ggplotify')
icc_plots <- lapply(1:22, function(i) {
as.grob(~ plot(model_grm, type = "trace", which.items = i, main = paste("Item", i)))
})
# === STEP 3: Mostrar parâmetros (discriminações e thresholds) ===
summary(model_grm)
# Diretório de saída (modifique conforme necessário)
output_dir <- "outputs/"
# Cria curva de informação por item
png(filename = file.path(output_dir, "item_information.png"), width = 800, height = 600)
plot(model_grm, type = "info")
dev.off()
# Cria curva de informação total com erro padrão
png(filename = file.path(output_dir, "test_information.png"), width = 800, height = 600)
plot(model_grm, type = "infoSE")
dev.off()
# Cria lista de gráficos convertidos para objetos 'ggplot'
info_plots <- lapply(1:22, function(i) {
p <- plot(model_grm, type = "info", which.items = i, main = paste("Item", i))
as.grob(~ plot(model_grm, type = "info", which.items = i, main = paste("Item", i)))
})
library(ggplotify)
# Cria lista de gráficos convertidos para objetos 'ggplot'
info_plots <- lapply(1:22, function(i) {
p <- plot(model_grm, type = "info", which.items = i, main = paste("Item", i))
as.grob(~ plot(model_grm, type = "info", which.items = i, main = paste("Item", i)))
})
# Diretório de saída (modifique conforme necessário)
output_dir <- "outputs/"
# Cria curva de informação por item
png(filename = file.path(output_dir, "item_information.png"), width = 800, height = 600)
plot(model_grm, type = "info")
dev.off()
# Cria curva de informação total com erro padrão
png(filename = file.path(output_dir, "test_information.png"), width = 800, height = 600)
plot(model_grm, type = "infoSE")
dev.off()
# Cria lista de gráficos convertidos para objetos 'ggplot'
info_plots <- lapply(1:22, function(i) {
p <- plot(model_grm, type = "info", which.items = i, main = paste("Item", i))
as.grob(~ plot(model_grm, type = "info", which.items = i, main = paste("Item", i)))
})
# Define layout do grid: 4 linhas × 6 colunas
png("outputs/grid_info_baseR.png", width = 2400, height = 1600, res = 150)
par(mfrow = c(4, 6), mar = c(4, 4, 2, 1))  # margens ajustadas
# Plota as curvas de informação por item
for (i in 1:22) {
plot(model_grm, type = "info", which.items = i, main = paste("Item", i))
}
dev.off()
# Define layout do grid: 4 linhas × 6 colunas
png("outputs/grid_info_baseR.png", width = 2400, height = 1600, res = 150)
par(mfrow = c(4, 6), mar = c(4, 4, 2, 1))  # margens ajustadas
# Plota as curvas de informação por item
for (i in 1:22) {
plot(model_grm, type = "info", which.items = i, main = paste("Item", i))
}
dev.off()
png("outputs/grid_icc_baseR.png", width = 2400, height = 1600, res = 150)
par(mfrow = c(4, 6), mar = c(4, 4, 2, 1))
for (i in 1:22) {
plot(model_grm, type = "trace", which.items = i, main = paste("Item", i))
}
dev.off()
# Cria figura com layout 4x6
png("outputs/grid_info_baseR.png", width = 2400, height = 1600, res = 150)
par(mfrow = c(4, 6), mar = c(4, 4, 2, 1))
# Loop sobre os itens (1 a 22)
for (i in 1:22) {
plot_item(model_grm, item = i, type = "info", main = paste("Item", i))
}
# Configura dispositivo gráfico
png("outputs/grid_info_baseR.png", width = 2400, height = 1600, res = 150)
par(mfrow = c(4, 6), mar = c(4, 4, 2, 1))  # define layout
# Plota curvas de informação dos 22 itens
for (i in 1:22) {
# Atenção: sem 'main', pois 'plot()' do mirt já insere
plot(model_grm, type = "info", which.items = i)
}
dev.off()
# Configura dispositivo gráfico
png("outputs/grid_item_information.png", width = 2400, height = 1600, res = 150)
par(mfrow = c(4, 6), mar = c(4, 4, 2, 1))
# Gera curvas de informação por item em grid
for (i in 1:22) {
itemplot(model_grm, item = i, type = "info", main = paste("Item", i))
}
dev.off()
mirt::plot(model_grm, type = "info", which.items = i)
# Configura dispositivo gráfico
png("outputs/grid_info_baseR.png", width = 2400, height = 1600, res = 150)
par(mfrow = c(4, 6), mar = c(4, 4, 2, 1))  # define layout
# Plota curvas de informação dos 22 itens
for (i in 1:22) {
# Atenção: sem 'main', pois 'plot()' do mirt já insere
mirt::plot(model_grm, type = "info", which.items = i)
}
dev.off()
plot(model_grm, type = "infoSE")
plot(model_grm, type = "info", which.items = i,
xlab = "Theta", ylab = "Item Information",
main = paste("Item", i), ylim = c(0, 1.5), col = "blue")
mirt::plot(model_grm, type = "info", which.items = 1)
mirt::plot(model_grm, type = "info", which.items = 2)
mirt::plot(model_grm, type = "info", which.items = 3)
# Gera sequência de theta
theta_vals <- seq(-4, 4, length.out = 201)
# Cria lista para armazenar dados de ICC por item
icc_data <- lapply(1:22, function(i) {
# Probabilidades por categoria para o item i
probs <- probtrace(model_grm, Theta = matrix(theta_vals), which.items = i)[[1]]
# Monta dataframe longo
tibble(
theta = rep(theta_vals, times = ncol(probs)),
probability = as.vector(probs),
category = factor(rep(colnames(probs), each = length(theta_vals))),
item = paste0("q", i)
)
})
# Gera sequência de theta
theta_vals <- seq(-4, 4, length.out = 201)
# Cria lista para armazenar dados de ICC por item
icc_data <- lapply(1:22, function(i) {
# Probabilidades por categoria para o item i
probs <- probtrace(model_grm, Theta = matrix(theta_vals))[[1]]
# Monta dataframe longo
tibble(
theta = rep(theta_vals, times = ncol(probs)),
probability = as.vector(probs),
category = factor(rep(colnames(probs), each = length(theta_vals))),
item = paste0("q", i)
)
})
# Gera vetor de valores de theta
theta_vals <- seq(-4, 4, length.out = 201)
# Probabilidades para todos os itens
prob_list <- probtrace(model_grm, Theta = matrix(theta_vals))
# Lista para armazenar data.frames por item
icc_data <- list()
for (i in seq_along(prob_list)) {
probs <- prob_list[[i]]  # matriz: 201 x 5 (para 5 categorias)
item_df <- as.data.frame(probs)
item_df$theta <- theta_vals
item_df_long <- pivot_longer(item_df, cols = -theta, names_to = "category", values_to = "probability")
item_df_long$item <- paste0("q", i)
icc_data[[i]] <- item_df_long
}
# Junta tudo
icc_df <- bind_rows(icc_data)
icc_df
# Cria estrutura para armazenar os ICCs
icc_data <- list()
for (i in seq_along(prob_list)) {
probs <- prob_list[[i]]
# Verifica se é matriz e se tem mesmo número de linhas que theta_vals
if (is.matrix(probs) && nrow(probs) == length(theta_vals)) {
item_df <- as.data.frame(probs)
item_df$theta <- theta_vals
# Reorganiza em formato longo
item_df_long <- pivot_longer(item_df, cols = -theta,
names_to = "category",
values_to = "probability")
item_df_long$item <- paste0("q", i)
icc_data[[i]] <- item_df_long
} else {
warning(paste("Item", i, "não retornou matriz esperada."))
}
# Junta todos os itens
icc_df <- bind_rows(icc_data)
icc_df
prob_list
library(tidyr)
library(dplyr)
library(ggplot2)
theta_vals <- seq(-4, 4, length.out = 201)
prob_matrix <- probtrace(model_grm, Theta = matrix(theta_vals))
# Transforma para data.frame com coluna theta
icc_df <- as.data.frame(prob_matrix)
icc_df$theta <- theta_vals
# Transforma para formato longo: item, category, probability
icc_long <- icc_df |>
pivot_longer(cols = -theta, names_to = "key", values_to = "probability") |>
separate(key, into = c("item", "category"), sep = "\\.P\\.") |>
mutate(
item = factor(item, levels = paste0("q", 1:22)),
category = factor(category, levels = as.character(1:5))
)
icc_long
ggplot(icc_long, aes(x = theta, y = probability, color = category)) +
geom_line(size = 0.9) +
facet_wrap(~ item, ncol = 4) +
scale_color_brewer(palette = "Dark2") +
labs(
title = "Item Characteristic Curves (ICC) – GRM",
x = expression(theta),
y = "Probability",
color = "Category"
) +
theme_minimal(base_size = 12)
ggsave("outputs/grid_icc_grm.png", width = 12, height = 14, dpi = 300)
# Sequência de theta
theta_vals <- seq(-4, 4, length.out = 201)
# Matriz de informação por item
info_matrix <- iteminfo(model_grm, Theta = matrix(theta_vals))
library(mirt)
library(ggplot2)
library(tidyr)
library(dplyr)
# Sequência de theta
theta_vals <- seq(-4, 4, length.out = 201)
# Obtem a informação por item usando plot() com verbose = TRUE
info_matrix <- plot(model_grm, type = "info", Theta = matrix(theta_vals), verbose = TRUE)
# Reestrutura os dados para plotar
iif_df <- as.data.frame(info_matrix)
iif_df
info_matrix
# === STEP 1: Carregar pacotes ===
library(mirt)
# === STEP 2: Ajustar modelo GRM unidimensional ===
model_grm <- mirt(db[, paste0("q", 1:22)],
model = 1,
itemtype = "graded",
SE = TRUE)
# === STEP 3: Mostrar parâmetros (discriminações e thresholds) ===
summary(model_grm)
# Diretório de saída (modifique conforme necessário)
output_dir <- "outputs/"
# Cria curva de informação por item
png(filename = file.path(output_dir, "item_information.png"), width = 800, height = 600)
plot(model_grm, type = "info")
dev.off()
# Cria curva de informação total com erro padrão
png(filename = file.path(output_dir, "test_information.png"), width = 800, height = 600)
plot(model_grm, type = "infoSE")
dev.off()
theta_vals <- seq(-4, 4, length.out = 201)
prob_matrix <- probtrace(model_grm, Theta = matrix(theta_vals))
# Transforma para data.frame com coluna theta
icc_df <- as.data.frame(prob_matrix)
icc_df$theta <- theta_vals
# Transforma para formato longo: item, category, probability
icc_long <- icc_df |>
pivot_longer(cols = -theta, names_to = "key", values_to = "probability") |>
separate(key, into = c("item", "category"), sep = "\\.P\\.") |>
mutate(
item = factor(item, levels = paste0("q", 1:22)),
category = factor(category, levels = as.character(1:5))
)
ggplot(icc_long, aes(x = theta, y = probability, color = category)) +
geom_line(size = 0.9) +
facet_wrap(~ item, ncol = 4) +
scale_color_brewer(palette = "Dark2") +
labs(
title = "Item Characteristic Curves (ICC) – GRM",
x = expression(theta),
y = "Probability",
color = "Category"
) +
theme_minimal(base_size = 12)
ggsave("outputs/grid_icc_grm.png", width = 12, height = 14, dpi = 300)
# Sequência de theta
theta_vals <- seq(-4, 4, length.out = 201)
# Reestrutura os dados para plotar
iif_df <- as.data.frame(info_matrix)
info_matrix
# Obtem a informação por item usando plot() com verbose = TRUE
info_m <- info_matrix$iteminfo(model_grm, Theta = matrix(theta_vals), verbose = TRUE)
info_matrix$iteminfo(model_grm, Theta = matrix(theta_vals), verbose = TRUE)
# Sequência de theta
theta_vals <- seq(-4, 4, length.out = 201)
# Lista para armazenar as informações por item
iif_data <- lapply(1:22, function(i) {
info_i <- iteminfo(model_grm, Theta = matrix(theta_vals), which.item = i)
tibble(
theta = theta_vals,
information = as.vector(info_i),
item = paste0("q", i)
)
})
# Vetor de theta
theta_vals <- seq(-4, 4, length.out = 201)
# Informação de todos os itens: matriz 201 x 22
info_matrix <- iteminfo(model_grm, Theta = matrix(theta_vals))
info_matrix
theta_vals
model_grm
iteminfo(model_grm, Theta = matrix(theta_vals))
iteminfo(model_grm)
iteminfo(model_grm, Theta = matrix(theta_vals))
theta_vals <- seq(-4, 4, length.out = 201)
# Usa o plot com verbose para retornar dados em vez de desenhar
info_matrix <- plot(model_grm, type = "info", Theta = matrix(theta_vals), verbose = TRUE)
info_matrix
info_df <- as.data.frame(info_matrix)
# === STEP 4: Estimar habilidade (theta) usando MLE ===
theta_estimates <- fscores(model_grm, method = "EAP", full.scores = TRUE)
theta_estimates
# Adiciona as estimativas de theta ao dataframe original
db <- db %>%
mutate(theta = theta_estimates)
db
View(db)
# === 1. Load required libraries ===
library(jsonlite)
library(tidyverse)
library(psych)
library(GPArotation)
library(lavaan)
library(mirt)
# === 2. Load and parse JSONL data ===
file_path <- 'outputs/mmbncas_llm_raw.jsonl'
lines <- read_lines(file_path)
data_list <- map(lines, ~ fromJSON(.x, simplifyVector = TRUE))
db <- bind_rows(data_list)
# === 3. Parse output column into items q1–q22 ===
parse_output <- function(x) {
tryCatch({
out <- fromJSON(x, simplifyVector = TRUE)
out_named <- as.list(out)
names(out_named) <- paste0('q', seq_along(out_named))
tibble(!!!out_named)
}, error = function(e) {
tibble(!!!set_names(rep(list(NA), 22), paste0('q', 1:22)))
})
}
output_parsed <- map(db$output, parse_output)
output_df <- bind_rows(output_parsed) |> mutate(across(everything(), as.numeric))
names(output_df) <- paste0('q', seq_len(ncol(output_df)))
db <- bind_cols(db, output_df)
# === 4. Summary of response distributions ===
item_cols <- paste0('q', 1:22)
item_mat <- db |> select(all_of(item_cols))
value_counts <- map_dfc(item_mat, ~ table(factor(.x, levels = 1:5)) |> as.integer())
names(value_counts) <- item_cols
value_counts <- value_counts |> mutate(value = 1:5) |> relocate(value)
# === 5. Reliability analysis (alpha and omega) ===
cronbach_alpha <- function(mat) {
mat <- na.omit(mat); if (ncol(mat) < 2 || nrow(mat) == 0) return(NA)
alpha(mat)$total[['raw_alpha']]
}
omega_total <- function(mat) {
mat <- na.omit(mat); vars <- apply(mat, 2, var, na.rm = TRUE); mat <- mat[, vars > 0, drop = FALSE]
if (ncol(mat) < 2 || nrow(mat) == 0) return(NA)
tryCatch({ suppressMessages(suppressWarnings(omega(mat, nfactors = 1, fm = 'ml', plot = FALSE)$omega.tot)) }, error = function(e) NA)
}
reliability_for <- function(cols, data_subset) {
mat <- data_subset[, cols, drop = FALSE]
list(alpha = cronbach_alpha(mat), omega = omega_total(mat))
}
dimensions <- list(
governance_strategy = paste0('q', 1:6),
operational_integration = paste0('q', 7:12),
sustainability_scalability = paste0('q', 13:22))
records <- list()
res <- reliability_for(item_cols, db)
records <- append(records, list(tibble(cut='full_scale', profile='all', dimension='all', n_items=22, alpha=res$alpha, omega=res$omega)))
for (prof in unique(db$profile)) {
res <- reliability_for(item_cols, db %>% filter(profile == prof))
records <- append(records, list(tibble(cut='full_scale', profile=prof, dimension='all', n_items=22, alpha=res$alpha, omega=res$omega)))
}
for (dim in names(dimensions)) {
cols <- dimensions[[dim]]
res <- reliability_for(cols, db)
records <- append(records, list(tibble(cut='dimension', profile='all', dimension=dim, n_items=length(cols), alpha=res$alpha, omega=res$omega)))
for (prof in unique(db$profile)) {
res <- reliability_for(cols, db %>% filter(profile == prof))
records <- append(records, list(tibble(cut='dimension', profile=prof, dimension=dim, n_items=length(cols), alpha=res$alpha, omega=res$omega)))
}
reliability_df <- bind_rows(records) |> mutate(across(c(alpha, omega), \(x) round(x, 3)))
# === 6. Parallel analysis and EFA ===
cor_spear <- cor(item_mat, method = 'spearman', use = 'pairwise.complete.obs')
eig_real <- eigen(cor_spear)$values
set.seed(123); eig_rand <- replicate(1000, {
rand <- matrix(sample(unlist(item_mat), replace = TRUE), ncol = ncol(item_mat))
eigen(cor(cor(rand, method = 'spearman')))$values }, simplify = 'matrix')
